Best individual (gen, episode):  (345, 5)  , reward:  0.5449180385856246  , action:  [0.99512739 0.50019147 0.9109601  0.18141494 0.96913662 0.49713205
 0.56362642 0.39788041]
fin_amplitude_left :  0.9951273949089985
fin_offset_left :  0.5001914690120375
frequency_left :  0.9109600957595335
phase_bias_left :  0.18141493999023792
fin_amplitude_right :  0.9691366191066215
fin_offset_right :  0.4971320547576582
frequency_right :  0.5636264229477349
phase_bias_right :  0.39788040919342355







    cma = CMA(mean=np.array([1, 0.5, 0.5, 0, 1, 0.5, 0.5, 0]),
            #   np.random.uniform(low=0,
            #                          high=1,
            #                          size=len(controller_parameterizer.get_parameter_labels())),
              sigma=0.1,
              bounds=bounds,
              population_size=10,    # has to be more than 1
              lr_adapt=True,
              seed=43
              )

    sim = OptimizerSimulation(
        task_config=config,
        robot_specification=robot_spec,
        parameterizer=controller_parameterizer,
        population_size=10,  # make sure this is a multiple of num_envs
        num_generations=500,
        outer_optimalization=cma,
        controller=CPG,
        skip_inner_optimalization=True,
        record_actions=True,
        action_spec=action_spec,
        num_envs=10,
        logging=False,
        )
